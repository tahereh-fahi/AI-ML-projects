{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tahereh-fahi/AI-ML-projects/blob/main/AnalyzingReview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "957effee",
      "cell_type": "markdown",
      "source": [
        "# Project 1: Automatic Analyzing review (Colab Notebook)\n",
        "\n",
        "**Status:** Private research notebook for academic evaluation.\n",
        "\n",
        "**Do not publish or make this notebook public.** MIT rules restrict sharing of project code.\n",
        "Grant view access *on request* to instructors/reviewers only.\n",
        "\n",
        "**Notebook generated:** 2025-08-24 18:02"
      ],
      "metadata": {
        "id": "957effee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import string\n",
        "import shutil\n",
        "\n",
        "# Data / math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Collections / utilities\n",
        "from collections import Counter, defaultdict, OrderedDict, deque\n",
        "from pathlib import Path\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report,\n",
        "    accuracy_score, roc_auc_score\n",
        ")\n",
        "from sklearn.preprocessing import label_binarize, normalize\n",
        "\n",
        "# NLTK (if you use text preprocessing)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Optional: tqdm progress bar, requests/bs4 for scraping, etc.\n",
        "# from tqdm import tqdm\n",
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# Compatibility flag for old code\n",
        "PYTHON3 = sys.version_info[0] >= 3\n"
      ],
      "metadata": {
        "id": "4PWbeQSHnsz4"
      },
      "id": "4PWbeQSHnsz4",
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "78bf6022",
      "cell_type": "markdown",
      "source": [
        "## 0) Environment & file setup\n",
        "- This notebook auto-downloads the dataset from a **shared Google Drive folder link** into `DATA_DIR` using `gdown --folder`.\n",
        "- If any files are still missing, it falls back to **manual upload**.\n",
        "- Keep the notebook **Restricted** (Share â†’ General access: *Restricted*)."
      ],
      "metadata": {
        "id": "78bf6022"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RUN ME FIRST: download & prepare dataset(s) from GitHub Releases -------\n",
        "import os, subprocess, hashlib, pathlib, zipfile, tarfile\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "def download(url: str, out_path: str):\n",
        "    print(f\"Downloading {url} -> {out_path}\")\n",
        "    subprocess.run(\n",
        "        [\"curl\", \"-L\", \"--fail\", \"--retry\", \"3\", \"--retry-delay\", \"3\", \"-o\", out_path, url],\n",
        "        check=True\n",
        "    )\n",
        "\n",
        "def sha256sum(path: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def verify(path: str, expected_sha256: str | None):\n",
        "    if not expected_sha256:\n",
        "        print(\"Skipping checksum verification.\")\n",
        "        return\n",
        "    actual = sha256sum(path)\n",
        "    if actual.lower() != expected_sha256.lower():\n",
        "        raise RuntimeError(f\"Checksum mismatch for {path}\\nexpected: {expected_sha256}\\nactual:   {actual}\")\n",
        "    print(\"Checksum OK:\", actual)\n",
        "\n",
        "def extract_if_archive(path: str, dest_dir=\"data\"):\n",
        "    p = pathlib.Path(path)\n",
        "    name = p.name.lower()\n",
        "    if name.endswith(\".zip\"):\n",
        "        with zipfile.ZipFile(path) as z:\n",
        "            z.extractall(dest_dir)\n",
        "        print(\"Unzipped to\", dest_dir)\n",
        "    elif name.endswith(\".tar.gz\") or name.endswith(\".tgz\"):\n",
        "        with tarfile.open(path, \"r:gz\") as t:\n",
        "            t.extractall(dest_dir)\n",
        "        print(\"Extracted tar.gz to\", dest_dir)\n",
        "    else:\n",
        "        print(\"Not an archive; nothing to extract.\")\n",
        "\n",
        "# === FILL THESE THREE LINES WITH YOUR RELEASE INFO ==========================\n",
        "URL = \"https://github.com/tahereh-fahi/Data/releases/download/v1.0.0/reviewAmazonData.zip\"\n",
        "OUT = \"data/my_dataset_v1.zip\"\n",
        "SHA256 = \"3ad20851fd564e93baad7d878e9d90fab314698206f5c228e54bf0bca8940135\"  # or set to None to skip\n",
        "# ============================================================================\n",
        "download(URL, OUT)\n",
        "verify(OUT, SHA256)\n",
        "extract_if_archive(OUT, \"data\")\n",
        "\n",
        "print(\"Ready. Your files are in ./data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fydwEB-KBmW",
        "outputId": "11748549-fc31-40a6-a49c-a23702d740ac"
      },
      "id": "1fydwEB-KBmW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/tahereh-fahi/Data/releases/download/v1.0.0/reviewAmazonData.zip -> data/my_dataset_v1.zip\n",
            "Checksum OK: 3ad20851fd564e93baad7d878e9d90fab314698206f5c228e54bf0bca8940135\n",
            "Unzipped to data\n",
            "Ready. Your files are in ./data\n"
          ]
        }
      ]
    },
    {
      "id": "bc94b662",
      "cell_type": "markdown",
      "source": [
        "## 1a) Utilities (data I/O, tuning helpers, feature inspection)"
      ],
      "metadata": {
        "id": "bc94b662"
      }
    },
    {
      "id": "3d186e4e",
      "cell_type": "code",
      "metadata": {
        "id": "3d186e4e"
      },
      "execution_count": null,
      "source": [
        "def load_data(path_data, extras=False):\n",
        "    \"\"\"\n",
        "    Returns a list of dict with keys:\n",
        "    * sentiment: +1 or -1 if the review was positive or negative, respectively\n",
        "    * text: the text of the review\n",
        "\n",
        "    Additionally, if the `extras` argument is True, each dict will also include the\n",
        "    following information:\n",
        "    * productId: a string that uniquely identifies each product\n",
        "    * userId: a string that uniquely identifies each user\n",
        "    * summary: the title of the review\n",
        "    * helpfulY: the number of users who thought this review was helpful\n",
        "    * helpfulN: the number of users who thought this review was NOT helpful\n",
        "    \"\"\"\n",
        "\n",
        "    global PYTHON3\n",
        "\n",
        "    basic_fields = {'sentiment', 'text'}\n",
        "    numeric_fields = {'sentiment', 'helpfulY', 'helpfulN'}\n",
        "\n",
        "    data = []\n",
        "    if PYTHON3:\n",
        "        f_data = open(path_data, encoding=\"latin1\")\n",
        "    else:\n",
        "        f_data = open(path_data)\n",
        "\n",
        "    for datum in csv.DictReader(f_data, delimiter='\\t'):\n",
        "        for field in list(datum.keys()):\n",
        "            if not extras and field not in basic_fields:\n",
        "                del datum[field]\n",
        "            elif field in numeric_fields and datum[field]:\n",
        "                datum[field] = int(datum[field])\n",
        "\n",
        "        data.append(datum)\n",
        "\n",
        "    f_data.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "def write_predictions(path_submit_data, preds):\n",
        "    if PYTHON3:\n",
        "        f_data = open(path_submit_data, encoding=\"latin1\")\n",
        "    else:\n",
        "        f_data = open(path_submit_data, 'rb')\n",
        "\n",
        "    reader = csv.DictReader(f_data, delimiter='\\t')\n",
        "    data = list(reader)\n",
        "\n",
        "    assert len(preds) == len(data), \\\n",
        "           'Expected {} predictions but {} were given.'.format(len(data), len(preds))\n",
        "\n",
        "    for pred, datum in zip(preds.astype(int), data):\n",
        "        assert pred == 1 or pred == -1, 'Invalid prediction: {}.'.format(pred)\n",
        "        datum['sentiment'] = pred\n",
        "    f_data.close()\n",
        "\n",
        "    if PYTHON3:\n",
        "        f_out = open(path_submit_data, 'w')\n",
        "    else:\n",
        "        f_out = open(path_submit_data, 'wb')\n",
        "\n",
        "    writer = csv.DictWriter(f_out, delimiter='\\t', fieldnames=reader.fieldnames)\n",
        "    writer.writeheader()\n",
        "    for datum in data:\n",
        "        writer.writerow(datum)\n",
        "    f_out.close()\n",
        "\n",
        "def tune(train_fn, param_vals, train_feats, train_labels, val_feats, val_labels):\n",
        "    train_accs = np.ndarray(len(param_vals))\n",
        "    val_accs = np.ndarray(len(param_vals))\n",
        "\n",
        "    for i, val in enumerate(param_vals):\n",
        "        theta, theta_0 = train_fn(train_feats, train_labels, val)\n",
        "\n",
        "        train_preds = classify(train_feats, theta, theta_0)\n",
        "        train_accs[i] = accuracy(train_preds, train_labels)\n",
        "\n",
        "        val_preds = classify(val_feats, theta, theta_0)\n",
        "        val_accs[i] = accuracy(val_preds, val_labels)\n",
        "\n",
        "    return train_accs, val_accs\n",
        "\n",
        "def tune_perceptron(*args):\n",
        "    return tune(perceptron, *args)\n",
        "\n",
        "def tune_avg_perceptron(*args):\n",
        "    return tune(average_perceptron, *args)\n",
        "\n",
        "def tune_pegasos_T(best_L, *args):\n",
        "    def train_fn(features, labels, T):\n",
        "        return pegasos(features, labels, T, best_L)\n",
        "    return tune(train_fn, *args)\n",
        "\n",
        "def tune_pegasos_L(best_T, *args):\n",
        "    def train_fn(features, labels, L):\n",
        "        return pegasos(features, labels, best_T, L)\n",
        "    return tune(train_fn, *args)\n",
        "\n",
        "def most_explanatory_word(theta, wordlist):\n",
        "    \"\"\"Returns the word associated with the bag-of-words feature having largest weight.\"\"\"\n",
        "    return [word for (theta_i, word) in sorted(zip(theta, wordlist))[::-1]]"
      ],
      "outputs": []
    },
    {
      "id": "d9f15641",
      "cell_type": "markdown",
      "source": [
        "## 2) Core algorithms & feature builders"
      ],
      "metadata": {
        "id": "d9f15641"
      }
    },
    {
      "id": "82aa9379",
      "cell_type": "code",
      "metadata": {
        "id": "82aa9379"
      },
      "execution_count": null,
      "source": [
        "def get_order(n_samples):\n",
        "    try:\n",
        "        with open(str(n_samples) + '.txt') as fp:\n",
        "            line = fp.readline()\n",
        "            return list(map(int, line.split(',')))\n",
        "    except FileNotFoundError:\n",
        "        random.seed(1)\n",
        "        indices = list(range(n_samples))\n",
        "        random.shuffle(indices)\n",
        "        return indices\n",
        "\n",
        "def hinge_loss_single(feature_vector, label, theta, theta_0):\n",
        "    \"\"\"\n",
        "    Finds the hinge loss on a single data point given specific classification\n",
        "    parameters.\n",
        "\n",
        "    Args:\n",
        "        feature_vector - A numpy array describing the given data point.\n",
        "        label - A real valued number, the correct classification of the data\n",
        "            point.\n",
        "        theta - A numpy array describing the linear classifier.\n",
        "        theta_0 - A real valued number representing the offset parameter.\n",
        "\n",
        "\n",
        "    Returns: A real number representing the hinge loss associated with the\n",
        "    given data point and parameters.\n",
        "    \"\"\"\n",
        "    # Your code\n",
        "    z = (np.dot(theta,feature_vector)+theta_0)*label\n",
        "    if z >=1:\n",
        "        return 0\n",
        "    else:\n",
        "        return (1-z)\n",
        "\n",
        "def hinge_loss_full(feature_matrix, labels, theta, theta_0):\n",
        "    \"\"\"\n",
        "    Finds the total hinge loss on a set of data given specific classification\n",
        "    parameters.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix - A numpy matrix describing the given data. Each row\n",
        "            represents a single data point.\n",
        "        labels - A numpy array where the kth element of the array is the\n",
        "            correct classification of the kth row of the feature matrix.\n",
        "        theta - A numpy array describing the linear classifier.\n",
        "        theta_0 - A real valued number representing the offset parameter.\n",
        "\n",
        "\n",
        "    Returns: A real number representing the hinge loss associated with the\n",
        "    given dataset and parameters. This number should be the average hinge\n",
        "    loss across all of the points in the feature matrix.\n",
        "    \"\"\"\n",
        "    # Your code\n",
        "    Z = (np.dot(feature_matrix, theta)+theta_0)*labels\n",
        "    hing_loss = 0\n",
        "    for z in Z:\n",
        "        if z<1:\n",
        "            hing_loss += (1-z)\n",
        "\n",
        "    hing_loss_totall = hing_loss / np.shape(feature_matrix)[0]\n",
        "\n",
        "    return hing_loss_totall\n",
        "\n",
        "def perceptron_single_step_update(\n",
        "        feature_vector,\n",
        "        label,\n",
        "        current_theta,\n",
        "        current_theta_0):\n",
        "    \"\"\"\n",
        "    Properly updates the classification parameter, theta and theta_0, on a\n",
        "    single step of the perceptron algorithm.\n",
        "\n",
        "    Args:\n",
        "        feature_vector - A numpy array describing a single data point.\n",
        "        label - The correct classification of the feature vector.\n",
        "        current_theta - The current theta being used by the perceptron\n",
        "            algorithm before this update.\n",
        "        current_theta_0 - The current theta_0 being used by the perceptron\n",
        "            algorithm before this update.\n",
        "\n",
        "    Returns: A tuple where the first element is a numpy array with the value of\n",
        "    theta after the current update has completed and the second element is a\n",
        "    real valued number with the value of theta_0 after the current updated has\n",
        "    completed.\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    if label * (np.dot(current_theta , feature_vector) + current_theta_0) <= 0:\n",
        "        theta = current_theta + label*feature_vector\n",
        "        theta_0 = current_theta_0 + label\n",
        "\n",
        "    else :\n",
        "        theta = current_theta\n",
        "        theta_0 = current_theta_0\n",
        "\n",
        "    return (theta, theta_0)\n",
        "\n",
        "def perceptron(feature_matrix, labels, T):\n",
        "    \"\"\"\n",
        "    Runs the full perceptron algorithm on a given set of data. Runs T\n",
        "    iterations through the data set, there is no need to worry about\n",
        "    stopping early.\n",
        "\n",
        "    NOTE: Please use the previously implemented functions when applicable.\n",
        "    Do not copy paste code from previous parts.\n",
        "\n",
        "    NOTE: Iterate the data matrix by the orders returned by get_order(feature_matrix.shape[0])\n",
        "\n",
        "    Args:\n",
        "        feature_matrix -  A numpy matrix describing the given data. Each row\n",
        "            represents a single data point.\n",
        "        labels - A numpy array where the kth element of the array is the\n",
        "            correct classification of the kth row of the feature matrix.\n",
        "        T - An integer indicating how many times the perceptron algorithm\n",
        "            should iterate through the feature matrix.\n",
        "\n",
        "    Returns: A tuple where the first element is a numpy array with the value of\n",
        "    theta, the linear classification parameter, after T iterations through the\n",
        "    feature matrix and the second element is a real number with the value of\n",
        "    theta_0, the offset classification parameter, after T iterations through\n",
        "    the feature matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # Your code here\n",
        "    theta = np.zeros(feature_matrix.shape[1])\n",
        "    theta_0 = 0\n",
        "    for t in range(T):\n",
        "        for i in get_order(feature_matrix.shape[0]):\n",
        "            # Your code here\n",
        "            theta , theta_0 = perceptron_single_step_update(feature_matrix[i,:] ,labels[i],theta,theta_0)\n",
        "\n",
        "    return (theta, theta_0)\n",
        "\n",
        "def average_perceptron(feature_matrix, labels, T):\n",
        "    \"\"\"\n",
        "    Runs the average perceptron algorithm on a given set of data. Runs T\n",
        "    iterations through the data set, there is no need to worry about\n",
        "    stopping early.\n",
        "\n",
        "    NOTE: Please use the previously implemented functions when applicable.\n",
        "    Do not copy paste code from previous parts.\n",
        "\n",
        "    NOTE: Iterate the data matrix by the orders returned by get_order(feature_matrix.shape[0])\n",
        "\n",
        "\n",
        "    Args:\n",
        "        feature_matrix -  A numpy matrix describing the given data. Each row\n",
        "            represents a single data point.\n",
        "        labels - A numpy array where the kth element of the array is the\n",
        "            correct classification of the kth row of the feature matrix.\n",
        "        T - An integer indicating how many times the perceptron algorithm\n",
        "            should iterate through the feature matrix.\n",
        "\n",
        "    Returns: A tuple where the first element is a numpy array with the value of\n",
        "    the average theta, the linear classification parameter, found after T\n",
        "    iterations through the feature matrix and the second element is a real\n",
        "    number with the value of the average theta_0, the offset classification\n",
        "    parameter, found after T iterations through the feature matrix.\n",
        "\n",
        "    Hint: It is difficult to keep a running average; however, it is simple to\n",
        "    find a sum and divide.\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    n = feature_matrix.shape[0]\n",
        "    d = feature_matrix.shape[1]\n",
        "\n",
        "    theta = np.zeros(d)\n",
        "    theta_0 = 0\n",
        "\n",
        "    theta_sum = np.zeros(d)\n",
        "    theta_0_sum = 0\n",
        "\n",
        "    for t in range(T):\n",
        "        for i in get_order(feature_matrix.shape[0]):\n",
        "            # Your code here\n",
        "            theta , theta_0 = perceptron_single_step_update(feature_matrix[i,:] ,labels[i],theta,theta_0)\n",
        "            theta_sum += theta\n",
        "            theta_0_sum += theta_0\n",
        "\n",
        "    theta_avg = theta_sum/(n*T)\n",
        "    theta_0_avg = theta_0_sum/(n*T)\n",
        "\n",
        "    return (theta_avg, theta_0_avg)\n",
        "\n",
        "def pegasos_single_step_update(\n",
        "        feature_vector,\n",
        "        label,\n",
        "        L,\n",
        "        eta,\n",
        "        current_theta,\n",
        "        current_theta_0):\n",
        "    \"\"\"\n",
        "    Properly updates the classification parameter, theta and theta_0, on a\n",
        "    single step of the Pegasos algorithm\n",
        "\n",
        "    Args:\n",
        "        feature_vector - A numpy array describing a single data point.\n",
        "        label - The correct classification of the feature vector.\n",
        "        L - The lamba value being used to update the parameters.\n",
        "        eta - Learning rate to update parameters.\n",
        "        current_theta - The current theta being used by the Pegasos\n",
        "            algorithm before this update.\n",
        "        current_theta_0 - The current theta_0 being used by the\n",
        "            Pegasos algorithm before this update.\n",
        "\n",
        "    Returns: A tuple where the first element is a numpy array with the value of\n",
        "    theta after the current update has completed and the second element is a\n",
        "    real valued number with the value of theta_0 after the current updated has\n",
        "    completed.\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    if label * (np.dot(current_theta , feature_vector) + current_theta_0) <= 1:\n",
        "\n",
        "        theta = (1- eta*L)* current_theta + eta *label*feature_vector\n",
        "        theta_0 = current_theta_0 + eta *label\n",
        "\n",
        "    else :\n",
        "\n",
        "        theta = (1- eta*L)* current_theta\n",
        "        theta_0 = current_theta_0\n",
        "\n",
        "    return (theta , theta_0)\n",
        "\n",
        "def pegasos(feature_matrix, labels, T, L):\n",
        "    \"\"\"\n",
        "    Runs the Pegasos algorithm on a given set of data. Runs T\n",
        "    iterations through the data set, there is no need to worry about\n",
        "    stopping early.\n",
        "\n",
        "    For each update, set learning rate = 1/sqrt(t),\n",
        "    where t is a counter for the number of updates performed so far (between 1\n",
        "    and nT inclusive).\n",
        "\n",
        "    NOTE: Please use the previously implemented functions when applicable.\n",
        "    Do not copy paste code from previous parts.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix - A numpy matrix describing the given data. Each row\n",
        "            represents a single data point.\n",
        "        labels - A numpy array where the kth element of the array is the\n",
        "            correct classification of the kth row of the feature matrix.\n",
        "        T - An integer indicating how many times the algorithm\n",
        "            should iterate through the feature matrix.\n",
        "        L - The lamba value being used to update the Pegasos\n",
        "            algorithm parameters.\n",
        "\n",
        "    Returns: A tuple where the first element is a numpy array with the value of\n",
        "    the theta, the linear classification parameter, found after T\n",
        "    iterations through the feature matrix and the second element is a real\n",
        "    number with the value of the theta_0, the offset classification\n",
        "    parameter, found after T iterations through the feature matrix.\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    d = feature_matrix.shape[1]\n",
        "\n",
        "    theta = np.zeros(d)\n",
        "    theta_0 = 0\n",
        "\n",
        "    t = 1\n",
        "\n",
        "    for tee in range(T):\n",
        "\n",
        "\n",
        "        for i in get_order(feature_matrix.shape[0]):\n",
        "\n",
        "            eta = 1/(np.sqrt(t))\n",
        "            theta , theta_0 = pegasos_single_step_update(feature_matrix[i,:] ,labels[i], L, eta, theta, theta_0)\n",
        "            t += 1\n",
        "\n",
        "    return (theta, theta_0)\n",
        "\n",
        "def classify(feature_matrix, theta, theta_0):\n",
        "    \"\"\"\n",
        "    A classification function that uses theta and theta_0 to classify a set of\n",
        "    data points.\n",
        "\n",
        "    Args:\n",
        "        feature_matrix - A numpy matrix describing the given data. Each row\n",
        "            represents a single data point.\n",
        "                theta - A numpy array describing the linear classifier.\n",
        "        theta - A numpy array describing the linear classifier.\n",
        "        theta_0 - A real valued number representing the offset parameter.\n",
        "\n",
        "    Returns: A numpy array of 1s and -1s where the kth element of the array is\n",
        "    the predicted classification of the kth row of the feature matrix using the\n",
        "    given theta and theta_0. If a prediction is GREATER THAN zero, it should\n",
        "    be considered a positive classification.\n",
        "    \"\"\"\n",
        "\n",
        "    yhat = np.dot(feature_matrix,theta)+theta_0\n",
        "\n",
        "    A = []\n",
        "    for yh in yhat:\n",
        "        if yh > 0:\n",
        "            A.append(1)\n",
        "        else:\n",
        "            A.append(-1)\n",
        "\n",
        "    return np.array(A)\n",
        "\n",
        "def classifier_accuracy(\n",
        "        classifier,\n",
        "        train_feature_matrix,\n",
        "        val_feature_matrix,\n",
        "        train_labels,\n",
        "        val_labels,\n",
        "        **kwargs):\n",
        "    \"\"\"\n",
        "    Trains a linear classifier and computes accuracy.\n",
        "    The classifier is trained on the train data. The classifier's\n",
        "    accuracy on the train and validation data is then returned.\n",
        "\n",
        "    Args:\n",
        "        classifier - A classifier function that takes arguments\n",
        "            (feature matrix, labels, **kwargs) and returns (theta, theta_0)\n",
        "        train_feature_matrix - A numpy matrix describing the training\n",
        "            data. Each row represents a single data point.\n",
        "        val_feature_matrix - A numpy matrix describing the validation\n",
        "            data. Each row represents a single data point.\n",
        "        train_labels - A numpy array where the kth element of the array\n",
        "            is the correct classification of the kth row of the training\n",
        "            feature matrix.\n",
        "        val_labels - A numpy array where the kth element of the array\n",
        "            is the correct classification of the kth row of the validation\n",
        "            feature matrix.\n",
        "        **kwargs - Additional named arguments to pass to the classifier\n",
        "            (e.g. T or L)\n",
        "\n",
        "    Returns: A tuple in which the first element is the (scalar) accuracy of the\n",
        "    trained classifier on the training data and the second element is the\n",
        "    accuracy of the trained classifier on the validation data.\n",
        "    \"\"\"\n",
        "    # train model\n",
        "    theta , theta_0 = classifier(train_feature_matrix, train_labels,**kwargs)\n",
        "\n",
        "    # train accuracy\n",
        "\n",
        "    yhat_train = np.dot(train_feature_matrix,theta)+theta_0\n",
        "    A1 = []\n",
        "    for yht in yhat_train:\n",
        "        if yht > 0:\n",
        "            A1.append(1)\n",
        "        else:\n",
        "            A1.append(-1)\n",
        "\n",
        "    train_labels_pred = np.array(A1)\n",
        "\n",
        "    train_accu = accuracy(train_labels_pred,train_labels)\n",
        "\n",
        "    # validation accuracy\n",
        "\n",
        "    yhat_val = np.dot(val_feature_matrix,theta)+theta_0\n",
        "    A2 = []\n",
        "    for yht in yhat_val:\n",
        "        if yht > 0:\n",
        "            A2.append(1)\n",
        "        else:\n",
        "            A2.append(-1)\n",
        "\n",
        "    val_labels_pred = np.array(A2)\n",
        "\n",
        "    val_accu = accuracy(val_labels_pred,val_labels)\n",
        "\n",
        "    return (train_accu, val_accu)\n",
        "\n",
        "def extract_words(input_string):\n",
        "    \"\"\"\n",
        "    Helper function for bag_of_words()\n",
        "    Inputs a text string\n",
        "    Returns a list of lowercase words in the string.\n",
        "    Punctuation and digits are separated out into their own words.\n",
        "    \"\"\"\n",
        "    for c in string.punctuation + string.digits:\n",
        "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
        "\n",
        "    return input_string.lower().split()\n",
        "\n",
        "def bag_of_words(texts):\n",
        "    \"\"\"\n",
        "    Inputs a list of string reviews\n",
        "    Returns a dictionary of unique unigrams occurring over the input\n",
        "\n",
        "    Feel free to change this code as guided by Problem 9\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    dictionary = {} # maps word to unique index\n",
        "    for text in texts:\n",
        "        word_list = extract_words(text)\n",
        "        for word in word_list:\n",
        "            if word not in dictionary:\n",
        "                dictionary[word] = len(dictionary)\n",
        "    return dictionary\n",
        "\n",
        "def extract_bow_feature_vectors(reviews, dictionary):\n",
        "    \"\"\"\n",
        "    Inputs a list of string reviews\n",
        "    Inputs the dictionary of words as given by bag_of_words\n",
        "    Returns the bag-of-words feature matrix representation of the data.\n",
        "    The returned matrix is of shape (n, m), where n is the number of reviews\n",
        "    and m the total number of entries in the dictionary.\n",
        "\n",
        "    Feel free to change this code as guided by Problem 9\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "\n",
        "    num_reviews = len(reviews)\n",
        "    feature_matrix = np.zeros([num_reviews, len(dictionary)])\n",
        "\n",
        "    for i, text in enumerate(reviews):\n",
        "        word_list = extract_words(text)\n",
        "        for word in word_list:\n",
        "            if word in dictionary:\n",
        "                feature_matrix[i, dictionary[word]] = 1\n",
        "    return feature_matrix\n",
        "\n",
        "def accuracy(preds, targets):\n",
        "    \"\"\"\n",
        "    Given length-N vectors containing predicted and target labels,\n",
        "    returns the percentage and number of correct predictions.\n",
        "    \"\"\"\n",
        "    return (preds == targets).mean()\n",
        "\n",
        "def classifier_accuracy_sanaz(\n",
        "        classifier,\n",
        "        train_feature_matrix,\n",
        "        val_feature_matrix,\n",
        "        train_labels,\n",
        "        val_labels,\n",
        "        **kwargs):\n",
        "    \"\"\"\n",
        "    Trains a linear classifier and computes accuracy.\n",
        "    The classifier is trained on the train data. The classifier's\n",
        "    accuracy on the train and validation data is then returned.\n",
        "\n",
        "    Args:\n",
        "        classifier - A classifier function that takes arguments\n",
        "            (feature matrix, labels, **kwargs) and returns (theta, theta_0)\n",
        "        train_feature_matrix - A numpy matrix describing the training\n",
        "            data. Each row represents a single data point.\n",
        "        val_feature_matrix - A numpy matrix describing the validation\n",
        "            data. Each row represents a single data point.\n",
        "        train_labels - A numpy array where the kth element of the array\n",
        "            is the correct classification of the kth row of the training\n",
        "            feature matrix.\n",
        "        val_labels - A numpy array where the kth element of the array\n",
        "            is the correct classification of the kth row of the validation\n",
        "            feature matrix.\n",
        "        **kwargs - Additional named arguments to pass to the classifier\n",
        "            (e.g. T or L)\n",
        "\n",
        "    Returns: A tuple in which the first element is the (scalar) accuracy of the\n",
        "    trained classifier on the training data and the second element is the\n",
        "    accuracy of the trained classifier on the validation data.\n",
        "    \"\"\"\n",
        "    # train model\n",
        "    theta , theta_0 = classifier(train_feature_matrix, train_labels,**kwargs)\n",
        "\n",
        "\n",
        "    return (theta, theta_0)\n",
        "\n",
        "def bag_of_words_sanaz(texts):\n",
        "    \"\"\"\n",
        "    Inputs a list of string reviews\n",
        "    Returns a dictionary of unique unigrams occurring over the input\n",
        "\n",
        "    Feel free to change this code as guided by Problem 9\n",
        "    \"\"\"\n",
        "    stopwords_list = load_data(os.path.join(\"/content/data\", \"reviews_train.tsv\"))\n",
        "\n",
        "\n",
        "    # Your code here\n",
        "    dictionary = {} # maps word to unique index\n",
        "    for text in texts:\n",
        "        word_list = extract_words(text)\n",
        "        for word in word_list:\n",
        "            if (word not in dictionary) and [word] not in stopwords_list :\n",
        "                dictionary[word] = len(dictionary)\n",
        "    return dictionary\n",
        "\n",
        "def extract_bow_feature_vectors_sanaz(reviews, dictionary):\n",
        "    \"\"\"\n",
        "    Inputs a list of string reviews\n",
        "    Inputs the dictionary of words as given by bag_of_words\n",
        "    Returns the bag-of-words feature matrix representation of the data.\n",
        "    The returned matrix is of shape (n, m), where n is the number of reviews\n",
        "    and m the total number of entries in the dictionary.\n",
        "\n",
        "    Feel free to change this code as guided by Problem 9\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "\n",
        "    num_reviews = len(reviews)\n",
        "    feature_matrix = np.zeros([num_reviews, len(dictionary)])\n",
        "\n",
        "    for i, text in enumerate(reviews):\n",
        "        word_list = extract_words(text)\n",
        "        for word in word_list:\n",
        "            if word in dictionary:\n",
        "                feature_matrix[i, dictionary[word]] = word_list.count(word)\n",
        "    return feature_matrix"
      ],
      "outputs": []
    },
    {
      "id": "9634ce3b",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "9634ce3b",
        "outputId": "eb410848-5531-4c80-9c7a-ab771a7a2326"
      },
      "execution_count": null,
      "source": [
        "train_data = load_data(os.path.join(\"/content/data\", \"reviews_train.tsv\"))\n",
        "val_data   = load_data(os.path.join(\"/content/data\", \"reviews_val.tsv\"))\n",
        "test_data  = load_data(os.path.join(\"/content/data\", \"reviews_test.tsv\"))\n",
        "\n",
        "train_texts, train_labels = zip(*((s[\"text\"], s[\"sentiment\"]) for s in train_data))\n",
        "val_texts,   val_labels   = zip(*((s[\"text\"],   s[\"sentiment\"]) for s in val_data))\n",
        "test_texts,  test_labels  = zip(*((s[\"text\"],   s[\"sentiment\"]) for s in test_data))\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "train_labels = np.array(train_labels); val_labels = np.array(val_labels); test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train: {len(train_texts)} | Val: {len(val_texts)} | Test: {len(test_texts)}\")\n",
        "print(\"Label balance (train):\", np.unique(train_labels, return_counts=True))\n",
        "print(\"Label balance (val):  \", np.unique(val_labels, return_counts=True))\n",
        "\n",
        "pd.DataFrame(train_data[:5])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 4000 | Val: 500 | Test: 500\n",
            "Label balance (train): (array([-1,  1]), array([2030, 1970]))\n",
            "Label balance (val):   (array([-1,  1]), array([259, 241]))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sentiment                                               text\n",
              "0         -1  The chips are okay Not near as flavorful as th...\n",
              "1         -1  I had high hopes for this, but it was bad.  Re...\n",
              "2         -1  I guess it's only one can since there is nothi...\n",
              "3         -1  \"Oatmeal Squares\" is in about the largest prin...\n",
              "4          1  I really enjoyed this flavor, this has a very ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90672430-b1cb-4a00-838f-b2ccfe6b76d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>The chips are okay Not near as flavorful as th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>I had high hopes for this, but it was bad.  Re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td>I guess it's only one can since there is nothi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td>\"Oatmeal Squares\" is in about the largest prin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>I really enjoyed this flavor, this has a very ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90672430-b1cb-4a00-838f-b2ccfe6b76d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-90672430-b1cb-4a00-838f-b2ccfe6b76d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-90672430-b1cb-4a00-838f-b2ccfe6b76d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5c8b3f08-97a6-42f3-90ca-21ce19cf450f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5c8b3f08-97a6-42f3-90ca-21ce19cf450f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5c8b3f08-97a6-42f3-90ca-21ce19cf450f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": -1,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          -1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I had high hopes for this, but it was bad.  Really bad.  The whole pan of cupcakes made from this had to be thrown out.  Very gritty and dense.\",\n          \"I really enjoyed this flavor, this has a very nice subtle coconut flavor that is not too sweet.  It's a hit in our household, I give them to my grand kids every time they come over and needless to say they keep coming back!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "id": "64f9d94f",
      "cell_type": "markdown",
      "source": [
        "## 3) Bagâ€‘ofâ€‘Words dictionaries and features\n",
        "We build classic unigram features with two variants:\n",
        "- **All tokens**\n",
        "- **Stopwords removed** (using `stopwords.txt`)"
      ],
      "metadata": {
        "id": "64f9d94f"
      }
    },
    {
      "id": "e2509a66",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2509a66",
        "outputId": "62f7d712-a3fa-4402-a7db-df528bb8ab2e"
      },
      "execution_count": null,
      "source": [
        "\n",
        "# With all tokens\n",
        "dictionary_all = bag_of_words(train_texts)\n",
        "Xtr_all = extract_bow_feature_vectors(train_texts, dictionary_all)\n",
        "Xva_all = extract_bow_feature_vectors(val_texts,   dictionary_all)\n",
        "Xte_all = extract_bow_feature_vectors(test_texts,  dictionary_all)\n",
        "print(\"All-tokens dictionary size:\", len(dictionary_all), \"| Xtr shape:\", Xtr_all.shape)\n",
        "\n",
        "# With stopwords removed\n",
        "dictionary_sw = bag_of_words_sanaz(train_texts)\n",
        "Xtr_sw = extract_bow_feature_vectors_sanaz(train_texts, dictionary_sw)\n",
        "Xva_sw = extract_bow_feature_vectors_sanaz(val_texts,   dictionary_sw)\n",
        "Xte_sw = extract_bow_feature_vectors_sanaz(test_texts,  dictionary_sw)\n",
        "print(\"Stopwordâ€‘filtered dictionary size:\", len(dictionary_sw), \"| Xtr shape:\", Xtr_sw.shape)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All-tokens dictionary size: 13234 | Xtr shape: (4000, 13234)\n",
            "Stopwordâ€‘filtered dictionary size: 13234 | Xtr shape: (4000, 13234)\n"
          ]
        }
      ]
    },
    {
      "id": "37af6601",
      "cell_type": "markdown",
      "source": [
        "## 4) Train & evaluate: Perceptron / Avg Perceptron / Pegasos"
      ],
      "metadata": {
        "id": "37af6601"
      }
    },
    {
      "id": "2894d221",
      "cell_type": "code",
      "metadata": {
        "id": "2894d221",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1a4004-0ac6-44e7-cfb2-4b1076ddb572"
      },
      "execution_count": null,
      "source": [
        "\n",
        "T = 10\n",
        "L = 0.01\n",
        "\n",
        "print(\"=== Using stopwordâ€‘filtered features ===\")\n",
        "pct_tr, pct_va = classifier_accuracy(perceptron, Xtr_sw, Xva_sw, train_labels, val_labels, T=T)\n",
        "print(f\"Perceptron â€” train: {pct_tr:.4f} | val: {pct_va:.4f}\")\n",
        "\n",
        "ap_tr, ap_va = classifier_accuracy(average_perceptron, Xtr_sw, Xva_sw, train_labels, val_labels, T=T)\n",
        "print(f\"Avg Perceptron â€” train: {ap_tr:.4f} | val: {ap_va:.4f}\")\n",
        "\n",
        "peg_tr, peg_va = classifier_accuracy(pegasos, Xtr_sw, Xva_sw, train_labels, val_labels, T=T, L=L)\n",
        "print(f\"Pegasos â€” train: {peg_tr:.4f} | val: {peg_va:.4f}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Using stopwordâ€‘filtered features ===\n",
            "Perceptron â€” train: 0.9123 | val: 0.7860\n",
            "Avg Perceptron â€” train: 0.8812 | val: 0.7940\n",
            "Pegasos â€” train: 0.8768 | val: 0.8040\n"
          ]
        }
      ]
    },
    {
      "id": "d78f22a2",
      "cell_type": "markdown",
      "source": [
        "## 5) Hyperparameter tuning (small sweep)"
      ],
      "metadata": {
        "id": "d78f22a2"
      }
    },
    {
      "id": "e6cb66cc",
      "cell_type": "code",
      "metadata": {
        "id": "e6cb66cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abbd45c3-4e32-41cb-d90d-455c9f14bd02"
      },
      "execution_count": null,
      "source": [
        "T_vals = [1, 3, 5, 10, 15, 25]\n",
        "L_vals = [1e-3, 3e-3, 1e-2, 3e-2, 1e-1]\n",
        "\n",
        "# Tune Pegasos T first (fixing lambda), then lambda (fixing T)\n",
        "tr, va = tune_pegasos_T(L, T_vals, Xtr_sw, train_labels, Xva_sw, val_labels)\n",
        "best_T = T_vals[int(np.argmax(va))]\n",
        "print(\"Tuning T:\", dict(zip(T_vals, va)), \"| best T:\", best_T)\n",
        "\n",
        "tr, va = tune_pegasos_L(best_T, L_vals, Xtr_sw, train_labels, Xva_sw, val_labels)\n",
        "best_L = L_vals[int(np.argmax(va))]\n",
        "print(\"Tuning lambda:\", dict(zip(L_vals, va)), \"| best lambda:\", best_L)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning T: {1: np.float64(0.708), 3: np.float64(0.742), 5: np.float64(0.744), 10: np.float64(0.804), 15: np.float64(0.776), 25: np.float64(0.78)} | best T: 10\n",
            "Tuning lambda: {0.001: np.float64(0.784), 0.003: np.float64(0.778), 0.01: np.float64(0.804), 0.03: np.float64(0.746), 0.1: np.float64(0.688)} | best lambda: 0.01\n"
          ]
        }
      ]
    },
    {
      "id": "141433fa",
      "cell_type": "markdown",
      "source": [
        "## 6) Final model & predictions"
      ],
      "metadata": {
        "id": "141433fa"
      }
    },
    {
      "id": "99b8a7d5",
      "cell_type": "code",
      "metadata": {
        "id": "99b8a7d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454d82b6-7a35-4f6d-ff67-49c9fcc7b4c5"
      },
      "execution_count": null,
      "source": [
        "final_theta, final_theta0 = pegasos(Xtr_sw, train_labels, T=best_T, L=best_L)\n",
        "test_preds = classify(Xte_sw, final_theta, final_theta0)\n",
        "test_acc = accuracy(test_preds, test_labels)\n",
        "print(f\"Final test accuracy (Pegasos, stopwordâ€‘filtered): {test_acc:.4f}\")\n",
        "\n",
        "# Write predictions for the competition/submit set\n",
        "submit_data = load_data(os.path.join(\"/content/data\", \"reviews_submit.tsv\"))\n",
        "submit_texts = [s[\"text\"] for s in submit_data]\n",
        "Xsub = extract_bow_feature_vectors_sanaz(submit_texts, dictionary_sw)\n",
        "submit_preds = classify(Xsub, final_theta, final_theta0)\n",
        "\n",
        "out_path = os.path.join(\"/content/data\", \"predictions_submit.tsv\")\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for y in submit_preds:\n",
        "        f.write(f\"{y}\\n\")\n",
        "print(\"Wrote:\", out_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test accuracy (Pegasos, stopwordâ€‘filtered): 0.7740\n",
            "Wrote: /content/data/predictions_submit.tsv\n"
          ]
        }
      ]
    },
    {
      "id": "e15a2307",
      "cell_type": "markdown",
      "source": [
        "## 7) Most explanatory words"
      ],
      "metadata": {
        "id": "e15a2307"
      }
    },
    {
      "id": "afee0b31",
      "cell_type": "code",
      "metadata": {
        "id": "afee0b31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25838668-97a0-43b3-aa82-f612e2c551ba"
      },
      "execution_count": null,
      "source": [
        "\n",
        "# Map weights back to words\n",
        "inv_vocab = {idx: w for w, idx in dictionary_sw.items()}\n",
        "wordlist = [inv_vocab[i] for i in range(len(inv_vocab))]\n",
        "\n",
        "# Sort words by weight (descending)\n",
        "sorted_pairs = sorted(zip(final_theta, wordlist), key=lambda x: x[0], reverse=True)\n",
        "top_pos = [w for _, w in sorted_pairs[:25]]\n",
        "top_neg = [w for _, w in sorted_pairs[-25:]]\n",
        "\n",
        "print(\"Top + words:\", top_pos)\n",
        "print(\"Top âˆ’ words:\", top_neg)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top + words: ['delicious', 'great', 'perfect', 'best', 'excellent', 'loves', 'favorite', 'love', 'find', 'tasty', 'with', ')', 'smooth', 'wonderful', 'recommend', 'beef', 'stores', 'glad', 'simply', 'who', 'healthy', 'without', 'free', 'worth', 'max']\n",
            "Top âˆ’ words: ['nibs', 'waste', 'horrible', 'were', 'tastes', 'corn', 'away', 'thought', 'raw', 'same', 'stick', 'guess', 'tasted', 'think', 'fine', 'rather', 'worst', 'ok', 't', 'money', 'unfortunately', 'not', 'bad', 'however', 'disappointed']\n"
          ]
        }
      ]
    },
    {
      "id": "1935d204",
      "cell_type": "markdown",
      "source": [
        "## 8) Reproducibility"
      ],
      "metadata": {
        "id": "1935d204"
      }
    },
    {
      "id": "4bcd3af4",
      "cell_type": "code",
      "metadata": {
        "id": "4bcd3af4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa247d31-0437-4ff0-8c1c-644061b84e56"
      },
      "execution_count": null,
      "source": [
        "# Verify the index order files are available\n",
        "print(\"First 20 of order(200):\", open(os.path.join(\"/content/data\", \"200.txt\")).read().split(\",\")[:20])\n",
        "print(\"First 20 of order(4000):\", open(os.path.join(\"/content/data\", \"4000.txt\")).read().split(\",\")[:20])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 of order(200): ['131', '181', '22', '172', '144', '92', '97', '187', '58', '93', '6', '70', '106', '68', '153', '168', '179', '199', '29', '46']\n",
            "First 20 of order(4000): ['2314', '3128', '1211', '666', '1266', '1627', '2831', '2975', '3163', '2836', '2967', '2585', '1291', '621', '3825', '2794', '61', '195', '1001', '3722']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e66007a",
        "outputId": "a3287f58-061e-423b-f6dd-60ae4aa6db1e"
      },
      "source": [
        "T_vals = [1, 3, 5, 10, 15, 25]\n",
        "L_vals = [1e-3, 3e-3, 1e-2, 3e-2, 1e-1]\n",
        "\n",
        "# Tune Pegasos T first (fixing lambda), then lambda (fixing T)\n",
        "tr, va = tune_pegasos_T(L, T_vals, Xtr_sw, train_labels, Xva_sw, val_labels)\n",
        "best_T = T_vals[int(np.argmax(va))]\n",
        "print(\"Tuning T:\", dict(zip(T_vals, va)), \"| best T:\", best_T)\n",
        "\n",
        "tr, va = tune_pegasos_L(best_T, L_vals, Xtr_sw, train_labels, Xva_sw, val_labels)\n",
        "best_L = L_vals[int(np.argmax(va))]\n",
        "print(\"Tuning lambda:\", dict(zip(L_vals, va)), \"| best lambda:\", best_L)"
      ],
      "id": "5e66007a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning T: {1: np.float64(0.708), 3: np.float64(0.742), 5: np.float64(0.744), 10: np.float64(0.804), 15: np.float64(0.776), 25: np.float64(0.78)} | best T: 10\n",
            "Tuning lambda: {0.001: np.float64(0.784), 0.003: np.float64(0.778), 0.01: np.float64(0.804), 0.03: np.float64(0.746), 0.1: np.float64(0.688)} | best lambda: 0.01\n"
          ]
        }
      ]
    }
  ]
}